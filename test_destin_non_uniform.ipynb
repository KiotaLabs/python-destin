{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Necesary Imports\n",
      "import cPickle as pickle\n",
      "from sklearn import svm\n",
      "from load_data import *\n",
      "from network import *\n",
      "import scipy.io as io"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# *****Define Parameters for the Network and nodes\n",
      "\n",
      "# Network Params\n",
      "num_layers = 4\n",
      "patch_mode = 'Adjacent'\n",
      "image_type = 'Color'\n",
      "network_mode = True\n",
      "# For a Node: specify Your Algorithm Choice and Corresponding parameters\n",
      "\n",
      "# ******************************************************************************************\n",
      "#\n",
      "#                           Incremental Clustering\n",
      "#\n",
      "num_nodes_per_layer = [[8, 8], [4, 4], [2, 2], [1, 1]]\n",
      "num_cents_per_layer = [25, 25, 25, 25]\n",
      "print \"Uniform DeSTIN with Clustering\"\n",
      "algorithm_choice = 'Clustering'\n",
      "alg_params = {'mr': 0.01, 'vr': 0.01, 'sr': 0.001, 'DIMS': [],\n",
      "             'CENTS': [], 'node_id': [],\n",
      "             'num_cents_per_layer': num_cents_per_layer}\n",
      "# ******************************************************************************************\n",
      "'''\n",
      "#  ******************************************************************************************\n",
      "\n",
      "#           Hierarchy Of AutoEncoders\n",
      "\n",
      "print \"Uniform DeSTIN with AutoEncoders\"\n",
      "num_nodes_per_layer = [[8, 8], [4, 4], [2, 2], [1, 1]]\n",
      "num_cents_per_layer = [25, 25, 25, 25]\n",
      "algorithm_choice = 'AutoEncoder'\n",
      "inp_size = 48\n",
      "hid_size = 100\n",
      "alg_params = [[inp_size, hid_size], [4 * hid_size, hid_size],\n",
      "             [4 * hid_size, hid_size], [4 * hid_size, hid_size]]\n",
      "#  ******************************************************************************************\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Load Data, 10 loads 5 batches in total 50,000\n",
      "# 1 to 5 load batch_1 to batch_5training images, 1 to five \n",
      "[data, labels] = loadCifar(10)\n",
      "del labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Declare a Network Object and load Training Data\n",
      "cifar_stat = load_cifar(10)\n",
      "DESTIN = Network(\n",
      "    num_layers, algorithm_choice, alg_params, num_nodes_per_layer, cifar_stat, patch_mode, image_type,)\n",
      "#, , , , cifar_stat, patch_mode='Adjacent', image_type='Color'\n",
      "DESTIN.setmode(network_mode)\n",
      "DESTIN.set_lowest_layer(0)\n",
      "# Load Data\n",
      "# Modify the location of the training data in file \"load_data.py\"\n",
      "\n",
      "# data = np.random.rand(5,32*32*3)\n",
      "# Initialize Network; there is is also a layer-wise initialization option\n",
      "DESTIN.init_network()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Train the Network\n",
      "print \"DeSTIN Training/with out Feature extraction\"\n",
      "for epoch in range(5):\n",
      "    for I in range(data.shape[0]):  # For Every image in the data set\n",
      "        if I % 1000 == 0:\n",
      "            print(\"Training Iteration Number %d\" % I)\n",
      "        for L in range(DESTIN.number_of_layers):\n",
      "            if L == 0:\n",
      "                img = data[I][:].reshape(32, 32, 3)\n",
      "                # This is equivalent to sharing centroids or kernels\n",
      "                DESTIN.layers[0][L].load_input(img, [4, 4])\n",
      "                DESTIN.layers[0][L].do_layer_learning()\n",
      "            else:\n",
      "                DESTIN.layers[0][L].load_input(\n",
      "                    DESTIN.layers[0][L - 1].nodes, [2, 2])\n",
      "                DESTIN.layers[0][L].do_layer_learning()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"DesTIN running/Feature Extraction/ over the Training Data\")\n",
      "network_mode = False\n",
      "DESTIN.setmode(network_mode)\n",
      "\n",
      "# Testin it over the training set\n",
      "[data, labels] = loadCifar(10)\n",
      "del labels\n",
      "for I in range(data.shape[0]):  # For Every image in the data set\n",
      "    if I % 1000 == 0:\n",
      "        print(\"Testing Iteration Number %d\" % I)\n",
      "    for L in range(DESTIN.number_of_layers):\n",
      "        if L == 0:\n",
      "            img = data[I][:].reshape(32, 32, 3)\n",
      "            DESTIN.layers[0][L].load_input(img, [4, 4])\n",
      "            DESTIN.layers[0][L].do_layer_learning()\n",
      "        else:\n",
      "            DESTIN.layers[0][L].load_input(\n",
      "                DESTIN.layers[0][L - 1].nodes, [2, 2])\n",
      "            DESTIN.layers[0][L].do_layer_learning()\n",
      "    DESTIN.update_belief_exporter()\n",
      "    if I in range(199, 50999, 200):\n",
      "        Name = 'train/' + str(I + 1) + '.txt'\n",
      "        file_id = open(Name, 'w')\n",
      "        pickle.dump(np.array(DESTIN.network_belief['belief']), file_id)\n",
      "        file_id.close()\n",
      "        # Get rid-off accumulated training beliefs\n",
      "        DESTIN.clean_belief_exporter()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Feature Extraction with the test set\")\n",
      "[data, labels] = loadCifar(6)\n",
      "del labels\n",
      "for I in range(data.shape[0]):  # For Every image in the data set\n",
      "    if I % 1000 == 0:\n",
      "        print(\"Testing Iteration Number %d\" % (I+50000))\n",
      "    for L in range(DESTIN.number_of_layers):\n",
      "        if L == 0:\n",
      "            img = data[I][:].reshape(32, 32, 3)\n",
      "            DESTIN.layers[0][L].load_input(img, [4, 4])\n",
      "            DESTIN.layers[0][L].do_layer_learning()  # Calculates belief for\n",
      "        else:\n",
      "            DESTIN.layers[0][L].load_input(\n",
      "                DESTIN.layers[0][L - 1].nodes, [2, 2])\n",
      "            DESTIN.layers[0][L].do_layer_learning()\n",
      "    DESTIN.update_belief_exporter()\n",
      "    if I in range(199, 10199, 200):\n",
      "        Name = 'test/' + str(I + 1) + '.txt'\n",
      "        file_id = open(Name, 'w')\n",
      "        pickle.dump(np.array(DESTIN.network_belief['belief']), file_id)\n",
      "        file_id.close()\n",
      "        # Get rid-off accumulated training beliefs\n",
      "        DESTIN.clean_belief_exporter()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Training With SVM\"\n",
      "print(\"Loading training and test labels\")\n",
      "[trainData, trainLabel] = loadCifar(10)\n",
      "del trainData\n",
      "[testData, testLabel] = loadCifar(6)\n",
      "del testData"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load Training and Test Data/Extracted from DeSTIN\n",
      "\n",
      "# here we do not use the whole set of feature extracted from DeSTIN\n",
      "# We use the features which are extracted from the top few layers\n",
      "print(\"Loading training and testing features\")\n",
      "trainData = np.array([])\n",
      "for I in range(199, 50000, 200):\n",
      "    Name = 'train/' + str(I + 1) + '.txt'\n",
      "    file_id = open(Name, 'r')\n",
      "    Temp = np.array(pickle.load(file_id))\n",
      "    file_id.close()\n",
      "    trainData = np.hstack((trainData, Temp))\n",
      "del Temp\n",
      "totLen = len(trainData)\n",
      "Width = int(totLen / 50000)\n",
      "trainData = trainData.reshape(50000, Width)\n",
      "trainData = trainData[:, 6400:8500]\n",
      "print trainData"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Training SVM\n",
      "SVM = svm.LinearSVC(C=10)\n",
      "# C=100, kernel='rbf')\n",
      "print \"Training the SVM\"\n",
      "trainLabel = np.squeeze(np.asarray(trainLabel).reshape(50000, 1))\n",
      "SVM.fit(trainData, trainLabel)\n",
      "print(\"Training Score = %f \" % float(100 * SVM.score(trainData, trainLabel, sample_weight=None)))\n",
      "#print(\"Training Accuracy = %f\" % (SVM.score(trainData, trainLabel) * 100))\n",
      "eff = {}\n",
      "eff['train'] = SVM.score(trainData, trainLabel) * 100\n",
      "del trainData\n",
      "\n",
      "testData = np.array([])\n",
      "for I in range(199, 10000, 200):\n",
      "    Name = 'test/' + str(I + 1) + '.txt'\n",
      "    file_id = open(Name, 'r')\n",
      "    Temp = np.array(pickle.load(file_id))\n",
      "    file_id.close()\n",
      "    testData = np.hstack((testData, Temp))\n",
      "totLen = len(testData)\n",
      "Width = int(totLen / 10000)\n",
      "testData = testData.reshape(10000, Width)\n",
      "testData = testData[:, 6400:8500]\n",
      "print testData\n",
      "del Temp\n",
      "print \"Predicting Test samples\"\n",
      "print(\"Test Score = %f\" % float(100 * SVM.score(testData, testLabel, sample_weight = None)))\n",
      "#print(\"Training Accuracy = %f\" % (SVM.score(testData, testLabel) * 100))\n",
      "eff['test'] = SVM.score(testData, testLabel) * 100\n",
      "io.savemat('accuracy.mat', eff)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}