{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Necesary Imports\n",
      "import cPickle as pickle\n",
      "from sklearn import svm\n",
      "from load_data import *\n",
      "from network import *\n",
      "import scipy.io as io"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# *****Define Parameters for the Network and nodes\n",
      "\n",
      "# Network Params\n",
      "num_layers = 4\n",
      "patch_mode = 'Adjacent'\n",
      "image_type = 'Color'\n",
      "network_mode = True\n",
      "# For a Node: specify Your Algorithm Choice and Corresponding parameters\n",
      "\n",
      "# ******************************************************************************************\n",
      "#\n",
      "#                           Incremental Clustering\n",
      "#\n",
      "num_nodes_per_layer = [[8, 8], [4, 4], [2, 2], [1, 1]]\n",
      "num_cents_per_layer = [25, 25, 25, 25]\n",
      "print \"Uniform DeSTIN with Clustering\"\n",
      "algorithm_choice = 'Clustering'\n",
      "alg_params = {'mr': 0.01, 'vr': 0.01, 'sr': 0.001, 'DIMS': [],\n",
      "             'CENTS': [], 'node_id': [],\n",
      "             'num_cents_per_layer': num_cents_per_layer}\n",
      "# ******************************************************************************************\n",
      "'''\n",
      "#  ******************************************************************************************\n",
      "\n",
      "#           Hierarchy Of AutoEncoders\n",
      "\n",
      "print \"Uniform DeSTIN with AutoEncoders\"\n",
      "num_nodes_per_layer = [[8, 8], [4, 4], [2, 2], [1, 1]]\n",
      "num_cents_per_layer = [25, 25, 25, 25]\n",
      "algorithm_choice = 'AutoEncoder'\n",
      "inp_size = 48\n",
      "hid_size = 100\n",
      "alg_params = [[inp_size, hid_size], [4 * hid_size, hid_size],\n",
      "             [4 * hid_size, hid_size], [4 * hid_size, hid_size]]\n",
      "#  ******************************************************************************************\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Uniform DeSTIN with Clustering\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "'\\n#  ******************************************************************************************\\n\\n#           Hierarchy Of AutoEncoders\\n\\nprint \"Uniform DeSTIN with AutoEncoders\"\\nnum_nodes_per_layer = [[8, 8], [4, 4], [2, 2], [1, 1]]\\nnum_cents_per_layer = [25, 25, 25, 25]\\nalgorithm_choice = \\'AutoEncoder\\'\\ninp_size = 48\\nhid_size = 100\\nalg_params = [[inp_size, hid_size], [4 * hid_size, hid_size],\\n             [4 * hid_size, hid_size], [4 * hid_size, hid_size]]\\n#  ******************************************************************************************\\n'"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Load Data, 10 loads 5 batches in total 50,000\n",
      "# 1 to 5 load batch_1 to batch_5training images, 1 to five \n",
      "[data, labels] = loadCifar(10)\n",
      "del labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Declare a Network Object and load Training Data\n",
      "cifar_stat = load_cifar(4)\n",
      "DESTIN = Network(\n",
      "    num_layers, algorithm_choice, alg_params, num_nodes_per_layer, cifar_stat, patch_mode, image_type,)\n",
      "#, , , , cifar_stat, patch_mode='Adjacent', image_type='Color'\n",
      "DESTIN.setmode(network_mode)\n",
      "DESTIN.set_lowest_layer(0)\n",
      "# Load Data\n",
      "# Modify the location of the training data in file \"load_data.py\"\n",
      "\n",
      "# data = np.random.rand(5,32*32*3)\n",
      "# Initialize Network; there is is also a layer-wise initialization option\n",
      "DESTIN.init_network()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Train the Network\n",
      "print \"DeSTIN Training/with out Feature extraction\"\n",
      "for epoch in range(5):\n",
      "    for I in range(data.shape[0]):  # For Every image in the data set\n",
      "        if I % 1000 == 0:\n",
      "            print(\"Training Iteration Number %d\" % I)\n",
      "        for L in range(DESTIN.number_of_layers):\n",
      "            if L == 0:\n",
      "                img = data[I][:].reshape(32, 32, 3)\n",
      "                # This is equivalent to sharing centroids or kernels\n",
      "                DESTIN.layers[0][L].load_input(img, [4, 4])\n",
      "                DESTIN.layers[0][L].do_layer_learning()\n",
      "            else:\n",
      "                DESTIN.layers[0][L].load_input(\n",
      "                    DESTIN.layers[0][L - 1].nodes, [2, 2])\n",
      "                DESTIN.layers[0][L].do_layer_learning()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "operands could not be broadcast together with shapes (1,48) (300,) ",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-5-78097af4fafc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;31m# This is equivalent to sharing centroids or kernels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mDESTIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mDESTIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_layer_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/habtegebrial/Desktop/python-destin/DeSTIN/python-destin/layer.pyc\u001b[0m in \u001b[0;36mload_input\u001b[0;34m(self, input_, R)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mJ\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     self.nodes[Nx][Ny].load_input(return_node_input(input_, [I, J], Ratio, self.patch_mode,\n\u001b[0;32m---> 32\u001b[0;31m                                                                  self.image_type))  # returns inputs to the node located at Position [Nx,Ny]\n\u001b[0m\u001b[1;32m     33\u001b[0m                     \u001b[0mNy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mNx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/habtegebrial/Desktop/python-destin/DeSTIN/python-destin/node.pyc\u001b[0m in \u001b[0;36mload_input\u001b[0;34m(self, In)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_number\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mIn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mIn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIn\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_std\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mIn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,48) (300,) "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DeSTIN Training/with out Feature extraction\n",
        "Training Iteration Number 0\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"DesTIN running/Feature Extraction/ over the Training Data\")\n",
      "network_mode = False\n",
      "DESTIN.setmode(network_mode)\n",
      "\n",
      "# Testin it over the training set\n",
      "[data, labels] = loadCifar(10)\n",
      "del labels\n",
      "for I in range(data.shape[0]):  # For Every image in the data set\n",
      "    if I % 1000 == 0:\n",
      "        print(\"Testing Iteration Number %d\" % I)\n",
      "    for L in range(DESTIN.number_of_layers):\n",
      "        if L == 0:\n",
      "            img = data[I][:].reshape(32, 32, 3)\n",
      "            DESTIN.layers[0][L].load_input(img, [4, 4])\n",
      "            DESTIN.layers[0][L].do_layer_learning()\n",
      "        else:\n",
      "            DESTIN.layers[0][L].load_input(\n",
      "                DESTIN.layers[0][L - 1].nodes, [2, 2])\n",
      "            DESTIN.layers[0][L].do_layer_learning()\n",
      "    DESTIN.update_belief_exporter()\n",
      "    if I in range(199, 50999, 200):\n",
      "        Name = 'train/' + str(I + 1) + '.txt'\n",
      "        file_id = open(Name, 'w')\n",
      "        pickle.dump(np.array(DESTIN.network_belief['belief']), file_id)\n",
      "        file_id.close()\n",
      "        # Get rid-off accumulated training beliefs\n",
      "        DESTIN.clean_belief_exporter()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Feature Extraction with the test set\")\n",
      "[data, labels] = loadCifar(6)\n",
      "del labels\n",
      "for I in range(data.shape[0]):  # For Every image in the data set\n",
      "    if I % 1000 == 0:\n",
      "        print(\"Testing Iteration Number %d\" % (I+50000))\n",
      "    for L in range(DESTIN.number_of_layers):\n",
      "        if L == 0:\n",
      "            img = data[I][:].reshape(32, 32, 3)\n",
      "            DESTIN.layers[0][L].load_input(img, [4, 4])\n",
      "            DESTIN.layers[0][L].do_layer_learning()  # Calculates belief for\n",
      "        else:\n",
      "            DESTIN.layers[0][L].load_input(\n",
      "                DESTIN.layers[0][L - 1].nodes, [2, 2])\n",
      "            DESTIN.layers[0][L].do_layer_learning()\n",
      "    DESTIN.update_belief_exporter()\n",
      "    if I in range(199, 10199, 200):\n",
      "        Name = 'test/' + str(I + 1) + '.txt'\n",
      "        file_id = open(Name, 'w')\n",
      "        pickle.dump(np.array(DESTIN.network_belief['belief']), file_id)\n",
      "        file_id.close()\n",
      "        # Get rid-off accumulated training beliefs\n",
      "        DESTIN.clean_belief_exporter()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Training With SVM\"\n",
      "print(\"Loading training and test labels\")\n",
      "[trainData, trainLabel] = loadCifar(10)\n",
      "del trainData\n",
      "[testData, testLabel] = loadCifar(6)\n",
      "del testData"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load Training and Test Data/Extracted from DeSTIN\n",
      "\n",
      "# here we do not use the whole set of feature extracted from DeSTIN\n",
      "# We use the features which are extracted from the top few layers\n",
      "print(\"Loading training and testing features\")\n",
      "trainData = np.array([])\n",
      "for I in range(199, 50000, 200):\n",
      "    Name = 'train/' + str(I + 1) + '.txt'\n",
      "    file_id = open(Name, 'r')\n",
      "    Temp = np.array(pickle.load(file_id))\n",
      "    file_id.close()\n",
      "    trainData = np.hstack((trainData, Temp))\n",
      "del Temp\n",
      "totLen = len(trainData)\n",
      "Width = int(totLen / 50000)\n",
      "trainData = trainData.reshape(50000, Width)\n",
      "trainData = trainData[:, 6400:8500]\n",
      "print trainData"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Training SVM\n",
      "SVM = svm.LinearSVC(C=10)\n",
      "# C=100, kernel='rbf')\n",
      "print \"Training the SVM\"\n",
      "trainLabel = np.squeeze(np.asarray(trainLabel).reshape(50000, 1))\n",
      "SVM.fit(trainData, trainLabel)\n",
      "print(\"Training Score = %f \" % float(100 * SVM.score(trainData, trainLabel, sample_weight=None)))\n",
      "#print(\"Training Accuracy = %f\" % (SVM.score(trainData, trainLabel) * 100))\n",
      "eff = {}\n",
      "eff['train'] = SVM.score(trainData, trainLabel) * 100\n",
      "del trainData\n",
      "\n",
      "testData = np.array([])\n",
      "for I in range(199, 10000, 200):\n",
      "    Name = 'test/' + str(I + 1) + '.txt'\n",
      "    file_id = open(Name, 'r')\n",
      "    Temp = np.array(pickle.load(file_id))\n",
      "    file_id.close()\n",
      "    testData = np.hstack((testData, Temp))\n",
      "totLen = len(testData)\n",
      "Width = int(totLen / 10000)\n",
      "testData = testData.reshape(10000, Width)\n",
      "testData = testData[:, 6400:8500]\n",
      "print testData\n",
      "del Temp\n",
      "print \"Predicting Test samples\"\n",
      "print(\"Test Score = %f\" % float(100 * SVM.score(testData, testLabel, sample_weight = None)))\n",
      "#print(\"Training Accuracy = %f\" % (SVM.score(testData, testLabel) * 100))\n",
      "eff['test'] = SVM.score(testData, testLabel) * 100\n",
      "io.savemat('accuracy.mat', eff)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'trainLabel' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-6-590bac056fbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# C=100, kernel='rbf')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Training the SVM\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainLabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mSVM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Score = %f \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mSVM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'trainLabel' is not defined"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training the SVM\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}